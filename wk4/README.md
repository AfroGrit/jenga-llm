# Evaluation and Monitoring

Hereâ€™s a summary of the video titled "LLM Zoomcamp 4.3 - Offline RAG Evaluation":

- **Objective**: The video focuses on evaluating the overall performance of a Retrieval-Augmented Generation (RAG) model by comparing answers generated by the model with original answers using cosine similarity.
  
- **Setup**: The evaluation uses previously generated datasets and synthetically created data. The setup includes indexing documents and generating embeddings for both the original and model-generated answers.

- **Method**: Cosine similarity is computed between the embeddings of the original and model-generated answers to assess their similarity. The evaluation process involves generating answers for all records in the dataset and analyzing these results.

- **Tools and Techniques**: The presenter uses tools such as GPT-4 and GPT-3.5 Turbo, noting that GPT-4 is more expensive and slower compared to GPT-3.5 Turbo. They also discuss parallel processing to speed up the evaluation.

- **Next Steps**: Compare the performance and cost-effectiveness of different models, with future videos expected to delve deeper into analyzing the results and metrics.


## Monitoring answer quality

## Offline vs Online (RAG) evaluation

## Generating data for offline RAG evaluation

## Offline RAG Evaluation
## Offline RAG evaluation: cosine similarity
## Offline RAG evaluation: LLM as a judge
## Capturing user feedback
## Capturing user feedback: part 2
## Monitoring the system
## Grafana
